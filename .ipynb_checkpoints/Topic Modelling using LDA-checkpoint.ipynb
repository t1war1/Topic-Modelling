{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314,) (11314,)\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gauravtiwari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington'], ['twilli', 'purdu', 'thoma', 'willi', 'subject', 'question', 'organ', 'purdu', 'univers', 'engin', 'network', 'distribut', 'line', 'folk', 'plus', 'final', 'give', 'ghost', 'weekend', 'start', 'life', 'sooo', 'market', 'machin', 'sooner', 'intend', 'look', 'pick', 'powerbook', 'mayb', 'bunch', 'question', 'hope', 'somebodi', 'answer', 'anybodi', 'know', 'dirt', 'round', 'powerbook', 'introduct', 'expect', 'hear', 'suppos', 'appear', 'summer', 'haven', 'hear', 'anymor', 'access', 'macleak', 'wonder', 'anybodi', 'info', 'anybodi', 'hear', 'rumor', 'price', 'drop', 'powerbook', 'line', 'like', 'one', 'go', 'recent', 'impress', 'display', 'probabl', 'swing', 'disk', 'feel', 'better', 'display', 'look', 'great', 'store', 'good', 'solicit', 'opinion', 'peopl', 'worth', 'take', 'disk', 'size', 'money', 'activ', 'display', 'realiz', 'real', 'subject', 'question', 'play', 'machin', 'store', 'breifli', 'figur', 'opinion', 'somebodi', 'actual', 'use', 'machin', 'daili', 'prove', 'help', 'hellcat', 'perform', 'thank', 'bunch', 'advanc', 'info', 'email', 'post', 'summari', 'news', 'read', 'time', 'premium', 'final', 'corner', 'willi', 'twilli', 'purdu', 'purdu', 'electr', 'engin', 'convict', 'danger', 'enemi', 'truth', 'lie', 'nietzsch']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from 'processed_docs' containing the number of times a word appears in the training set\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples where token id\n",
    "# is the index of word in the dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 2),\n",
       "  (18, 1),\n",
       "  (19, 2),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1)],\n",
       " [(13, 1),\n",
       "  (18, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (27, 1),\n",
       "  (34, 1),\n",
       "  (36, 1),\n",
       "  (38, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 2),\n",
       "  (51, 5),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 2),\n",
       "  (57, 1),\n",
       "  (58, 2),\n",
       "  (59, 2),\n",
       "  (60, 1),\n",
       "  (61, 2),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 3),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1),\n",
       "  (83, 2),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 3),\n",
       "  (87, 1),\n",
       "  (88, 4)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 13 (\"host\") appears 1 time.\n",
      "Word 18 (\"line\") appears 1 time.\n",
      "Word 24 (\"nntp\") appears 1 time.\n",
      "Word 25 (\"organ\") appears 1 time.\n",
      "Word 27 (\"post\") appears 1 time.\n",
      "Word 29 (\"rest\") appears 1 time.\n",
      "Word 34 (\"subject\") appears 1 time.\n",
      "Word 37 (\"thing\") appears 5 time.\n",
      "Word 115 (\"give\") appears 1 time.\n",
      "Word 128 (\"like\") appears 1 time.\n",
      "Word 138 (\"peopl\") appears 1 time.\n",
      "Word 212 (\"write\") appears 1 time.\n",
      "Word 221 (\"clear\") appears 1 time.\n",
      "Word 338 (\"say\") appears 1 time.\n",
      "Word 386 (\"think\") appears 1 time.\n",
      "Word 437 (\"refer\") appears 1 time.\n",
      "Word 452 (\"true\") appears 1 time.\n",
      "Word 504 (\"technolog\") appears 1 time.\n",
      "Word 564 (\"christian\") appears 1 time.\n",
      "Word 582 (\"exampl\") appears 1 time.\n",
      "Word 607 (\"jew\") appears 1 time.\n",
      "Word 612 (\"lead\") appears 1 time.\n",
      "Word 615 (\"littl\") appears 3 time.\n",
      "Word 664 (\"wors\") appears 2 time.\n",
      "Word 920 (\"keith\") appears 3 time.\n",
      "Word 933 (\"punish\") appears 1 time.\n",
      "Word 1016 (\"california\") appears 1 time.\n",
      "Word 1083 (\"institut\") appears 1 time.\n",
      "Word 1147 (\"similar\") appears 1 time.\n",
      "Word 1244 (\"allan\") appears 1 time.\n",
      "Word 1245 (\"anti\") appears 1 time.\n",
      "Word 1246 (\"arriv\") appears 1 time.\n",
      "Word 1247 (\"austria\") appears 1 time.\n",
      "Word 1248 (\"caltech\") appears 2 time.\n",
      "Word 1249 (\"distinguish\") appears 1 time.\n",
      "Word 1250 (\"german\") appears 1 time.\n",
      "Word 1251 (\"germani\") appears 3 time.\n",
      "Word 1252 (\"hitler\") appears 1 time.\n",
      "Word 1253 (\"imperail\") appears 1 time.\n",
      "Word 1254 (\"livesey\") appears 2 time.\n",
      "Word 1255 (\"motto\") appears 2 time.\n",
      "Word 1256 (\"order\") appears 1 time.\n",
      "Word 1257 (\"pasadena\") appears 1 time.\n",
      "Word 1258 (\"pevas\") appears 1 time.\n",
      "Word 1259 (\"pompous\") appears 1 time.\n",
      "Word 1260 (\"popul\") appears 1 time.\n",
      "Word 1261 (\"rank\") appears 1 time.\n",
      "Word 1262 (\"schneider\") appears 1 time.\n",
      "Word 1263 (\"semit\") appears 1 time.\n",
      "Word 1264 (\"social\") appears 1 time.\n",
      "Word 1265 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA on the bad of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics=number of requested latent topics to be extracted from the training corpus\n",
    "# id2word is a mapping from word ids (integers) to words (strings) \n",
    "# workers is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "# passes is the number of training passes through the corpus.\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"encrypt\" + 0.006*\"line\" + 0.006*\"subject\" + 0.005*\"govern\" + 0.005*\"secur\" + 0.005*\"chip\" + 0.005*\"write\" + 0.005*\"israel\" + 0.005*\"organ\" + 0.005*\"isra\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.012*\"game\" + 0.010*\"team\" + 0.007*\"play\" + 0.007*\"line\" + 0.007*\"year\" + 0.007*\"subject\" + 0.006*\"organ\" + 0.006*\"player\" + 0.005*\"hockey\" + 0.005*\"think\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"write\" + 0.007*\"organ\" + 0.007*\"line\" + 0.007*\"articl\" + 0.007*\"subject\" + 0.007*\"peopl\" + 0.005*\"like\" + 0.004*\"post\" + 0.004*\"know\" + 0.004*\"think\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.016*\"line\" + 0.015*\"subject\" + 0.015*\"organ\" + 0.012*\"write\" + 0.010*\"articl\" + 0.009*\"post\" + 0.008*\"univers\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"like\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.014*\"line\" + 0.013*\"organ\" + 0.013*\"subject\" + 0.011*\"write\" + 0.010*\"post\" + 0.010*\"articl\" + 0.009*\"host\" + 0.008*\"nntp\" + 0.006*\"univers\" + 0.006*\"like\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.011*\"space\" + 0.009*\"nasa\" + 0.005*\"work\" + 0.004*\"organ\" + 0.004*\"program\" + 0.004*\"line\" + 0.004*\"subject\" + 0.004*\"orbit\" + 0.004*\"time\" + 0.004*\"year\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.010*\"peopl\" + 0.008*\"think\" + 0.007*\"say\" + 0.007*\"know\" + 0.006*\"write\" + 0.006*\"christian\" + 0.005*\"subject\" + 0.005*\"right\" + 0.005*\"believ\" + 0.005*\"line\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.013*\"line\" + 0.012*\"subject\" + 0.010*\"organ\" + 0.010*\"window\" + 0.008*\"file\" + 0.007*\"post\" + 0.006*\"write\" + 0.006*\"card\" + 0.005*\"host\" + 0.005*\"univers\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: help\n",
      "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
      "Lines: 13\n",
      "\n",
      "Hello All!\n",
      "\n",
      "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
      "prior to starting Windows - this makes getting into Windows quite slow if you\n",
      "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
      "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
      "maybe make something like a PIF file to load them only when you enter the\n",
      "applications that need fonts?  Any ideas?\n",
      "\n",
      "\n",
      "Chris\n",
      "\n",
      " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.901461124420166\t Topic: 0.013*\"line\" + 0.012*\"subject\" + 0.010*\"organ\" + 0.010*\"window\" + 0.008*\"file\"\n",
      "Score: 0.08107218891382217\t Topic: 0.010*\"peopl\" + 0.008*\"think\" + 0.007*\"say\" + 0.007*\"know\" + 0.006*\"write\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_test.target_names[newsgroups_test.target[num]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with 'x'% probability to the X category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
